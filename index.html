<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="MonarchRT: Efficient Attention for Real-Time Video Generation">
  <meta property="og:title" content="MonarchRT"/>
  <meta property="og:description" content="MonarchRT: Efficient Attention for Real-Time Video Generation"/>
  <meta property="og:image" content="static/images/icons/Bolt.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="MonarchRT">
  <meta name="twitter:description" content="MonarchRT: Efficient Attention for Real-Time Video Generation">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="video diffusion, diffusion transformers, attention, structured matrices, Monarch">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MonarchRT: Efficient Attention for Real-Time Video Generation</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {
        Macros: {
          softmax: "\\mathrm{softmax}"
        }
      }
    });
  </script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <style>
    @font-face {
      font-family: 'TriForceFont';
      src: url('static/Triforce.ttf') format('truetype');
    }

    .hero.is-light {
      background-color: #ffffff !important;
    }

    body {
      background-color: #ffffff;
    }

    .container.is-fluid {
      margin-left: 15px;
      margin-right: 15px;
      max-width: none;
    }

    .hero .hero-body {
      padding: 3rem 0;
    }

    .section {
      padding: 3rem 0;
    }

    .column.is-full-width {
      padding: 0 15px;
    }

    .note-box {
      background: #ffffff;
      border-left: 4px solid #4b6cb7;
      border-radius: 6px;
      padding: 16px 24px;
      font-size: 16px;
      line-height: 1.6;
      color: #333;
    }

    .takeaway {
      background: #ffffff;
      border: 1px solid rgba(75,108,183,0.35);
      border-left: 4px solid #4b6cb7;
      border-radius: 6px;
      padding: 14px 18px;
      margin-top: 14px;
      margin-bottom: 14px;
      color: #333;
    }

    .figure-card {
      flex: 1 1 48%;
      background: #fff;
      padding: 16px;
      border-radius: 8px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.05);
    }

    .figure-title {
      text-align: center;
      color: #4b6cb7;
    }

    .monospace {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.95em;
    }

    .tight-table td, .tight-table th {
      padding: 0.5rem 0.6rem;
      vertical-align: middle;
    }

    /* --- Horizontal scrolling stacked-video comparison (rotated row labels) --- */
    .video-compare {
      --thumb-w: 360px;     /* width of each column */
      --thumb-gap: 18px;    /* gap between columns */
      --row-gap: 14px;      /* gap between the two rows */
      --label-w: 64px;      /* narrow label column since we rotate text */
      --vid-h: calc(var(--thumb-w) * 9 / 16); /* assumes 16:9 */
    }

    .video-compare__grid {
      display: grid;
      grid-template-columns: var(--label-w) 1fr;
      column-gap: 12px;
      align-items: start;
    }

    /* Left labels column */
    .video-compare__labels {
      display: flex;
      flex-direction: column;
      gap: var(--row-gap);
      padding-top: 6px; /* alignment tweak */
    }

    /* Each label box matches the video height */
    .video-compare__label {
      height: var(--vid-h);
      width: var(--label-w);
      display: flex;
      align-items: center;
      justify-content: center;

      background: rgba(75,108,183,0.10);
      border: 1px solid rgba(75,108,183,0.25);
      border-radius: 10px;
      overflow: hidden;
    }

    /* Rotate the label text counterclockwise */
    .video-compare__label span {
      display: inline-block;
      transform: rotate(-90deg);
      transform-origin: center;
      white-space: nowrap;

      font-weight: 800;
      color: #1f2d3d;
      line-height: 1;
    }

    /* Scroll area */
    .video-compare__scroll {
      overflow-x: auto;
      overflow-y: hidden;
      -webkit-overflow-scrolling: touch;
      padding-bottom: 6px;
      border-radius: 10px;
    }

    .video-compare__strip {
      display: flex;
      gap: var(--thumb-gap);
      align-items: flex-start;
      padding: 6px 2px;

      scroll-snap-type: x mandatory;
    }

    /* Each column = 2 stacked videos */
    .video-compare__col {
      flex: 0 0 auto;
      width: var(--thumb-w);
      display: flex;
      flex-direction: column;
      gap: var(--row-gap);
      scroll-snap-align: start;
    }

    .video-compare__vid {
      width: 100%;
      aspect-ratio: 16 / 9;
      object-fit: cover; /* or 'contain' */
      border-radius: 10px;
      background: #000;
      box-shadow: 0 4px 12px rgba(0,0,0,0.06);
    }

    @media (max-width: 768px) {
      .video-compare {
        --thumb-w: 280px;
        --label-w: 54px;
      }
      .video-compare__label span {
        font-size: 0.95rem;
      }
    }
  </style>
</head>

<body>

<!-- Section: Header Titlepage -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-fluid">
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h1 class="title is-2 publication-title" style="display: inline;">MonarchRT: Efficient Attention for Real-Time Video Generation</h1>

          <br><br>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://krishagarwal.github.io/" target="_blank">Krish Agarwal</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://dreaming-panda.github.io/" target="_blank">Zhuoming Chen</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://wdlctc.github.io/" target="_blank">Cheng Luo</a>,</span>
            <span class="author-block"><a href="https://brianchen1129.github.io/" target="_blank">Yongqi Chen</a><sup>3</sup>,</span>
            <span class="author-block"><a href="https://zhenghaizhong.com/" target="_blank">Haizhong Zheng</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.xunhuang.me/" target="_blank">Xun Huang</a><sup>3</sup>,</span>
            <span class="author-block"><a href="https://cse.buffalo.edu/faculty/atri/" target="_blank">Atri Rudra</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.andrew.cmu.edu/user/beidic/" target="_blank">Beidi Chen</a><sup>1</sup></span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 4px;">
            <span class="affilation">
              <small>
                <sup>1</sup>Carnegie Mellon University &nbsp;
                <sup>2</sup>University at Buffalo &nbsp;
                <sup>3</sup>Morpheus AI
              </small>
            </span>
          </div>

          <div class="column has-text-centered" style="margin-top: 10px;">
            <span class="link-block">
              <a href="https://arxiv.org/abs/2602.12271" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/Infini-AI-Lab/MonarchRT" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>
            </span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section: Highlight Figures -->
<!-- <section class="section" style="background: #f7faff; padding-top: 32px; padding-bottom: 16px;"> -->
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div style="display: flex; gap: 24px; flex-wrap: wrap; justify-content: center;">
          <!-- <div class="container is-max-desktop"> -->
            <div class="content has-text-justified">
              <video controls preload="metadata" autoplay muted loop playsinline style="width: 100%; height: auto; border-radius: 8px;">
                <source src="static/videos/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
<!-- </section> -->


<!-- Section: TL;DR -->
<section class="section hero is-light">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <strong style="font-weight: 900;color: #0f598a">TL;DR:</strong>
            We introduce <strong>MonarchRT</strong>, a method that sparsely parameterizes attention maps in video generation DiTs as Monarch matrices. Specifically, it addresses three key limitations of existing
            work on Monarch-style attention to make it feasible for video:
            <strong style="color: #4b6cb7;">(1)</strong> video dimension <strong>block alignment</strong>, <strong style="color: #4b6cb7;">(2)</strong> monotonic refinement with compute via <strong>tiled Monarch</strong> parameterization, and
            <strong style="color: #4b6cb7;">(3)</strong> linear iterative refinement overhead, mitigated by <strong>1-iteration + finetuning</strong> and custom Triton kernels.
            Empirically, MonarchRT reaches up to <strong>95% effective attention sparsity</strong> with no quality loss and achieves speedups over FlashAttention(-2, -3, -4) kernels
            in the range of <strong style="color: #27ae60;">1.4–11.8$\times$</strong>. Notably, <strong style="color: #27ae60;">this enables, for the first time, true real-time 16 FPS video generation with Self-Forcing on a single RTX 5090.</strong>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Section: Observations / Model -->
<section class="section hero is-light" id="analysis">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/MathematicsCompass.png" style="height: 43px; display: inline; vertical-align:text-top;"/>
          &nbsp; Motivation and Observations: Structure of 3D Attention
        </h2>

        <div class="content has-text-justified">
          <span id="intro"></span>

          <p>
            3D self-attention is a major computational bottleneck for Diffusion Transformers (DiTs) in video generation. Many recent works have shown that
            video attention maps are highly redundant, and costs can be significantly reduced through various <strong>sparse attention</strong> approximations.
            However, these methods generally target bidirectional models with 50 or more diffusion steps, which can easily hide approximation errors; meanwhile,
            <strong>real-time video</strong> sits in a harsh regime consisting of <strong>few-step</strong> diffusion and <strong>autoregressive</strong> generation,
            where approximation errors can propagate across steps/frames.
          </p>
          <p>
            In this setting, we observe that sparsification can be fundamentally brittle, as video attention is not reliably sparse. Instead, it combines <em style="color: #e0710a;">(i) strong spatiotemporal periodic structure</em>,
            <em style="color: #dd0e0e;">(ii) sparse semantic correspondences</em>, and <em style="color: #7911db;">(iii) intermittent dense mixing</em>, which exceed the representational capacity of even oracle top-$k$ attention.
          </p>

          <div style="text-align: center; margin-top: 18px;">
            <img src="static/images/motivating_example_a.png" alt="Oracle top-k vs. Monarch approximation error" style="max-width: 90%; border-radius: 8px;">
            <div style="margin-top: 8px; font-size: 14px; color: #555; max-width: 90%; margin-left: auto; margin-right: auto;">
              <em>Figure 1: MSE of attention map approximation using oracle top-$k$ and Monarch on Self-Forcing for layer 0 head 0 (L0H0) and layer 8 head 7 (L8H7) for varying sparsity levels. Even oracle top-$k$ can incur large errors at high sparsity, while Monarch preserves structure.</em>
            </div>
          </div>

          <p style="margin-top: 24px;">
            Consider a video latent with frames $f$, height $h$, width $w$ (token count $N = f h w$).
            Writing indices as $(f_0,h_0,w_0)$ and $(f_1,h_1,w_1)$, we model the (post-softmax) attention matrix $\mathbf{A}$ as
          </p>

          <p style="margin-bottom: 0px;">
            $$\mathbf{A}_{(f_0,h_0,w_0),(f_1,h_1,w_1)} = \mathbf{D}_{(f_0,h_0,w_0),(f_1,h_1,w_1)} + \mathbf{S}_{(f_0,h_0,w_0),(f_1,h_1,w_1)} + \varepsilon,$$
            $$\mathbf{D}_{(f_0,h_0,w_0),(f_1,h_1,w_1)} = d_f(f_0,f_1)\, d_h(h_0,h_1)\, d_w(w_0,w_1).$$
          </p>
          <p>
            Here $\mathbf{D}$ captures <em>separable positional structure</em> (smoothly decaying distances along
            time/height/width), while $\mathbf{S}$ captures <em>sparse semantic links</em> (retrieval-like spikes)
            that are independent of position.
          </p>

          <div class="takeaway">
            <strong>Why top-$k$ can fail:</strong> even if $\mathbf{S}$ is sparse, $\mathbf{D}$ is not.
            Some heads may require a large fraction of tokens to recover most of the attention mass, so enforcing a hard
            $k$ can delete essential periodic mixing.
          </div>

          <div style="text-align: center; margin-top: 18px;">
            <img src="static/images/3D_attention_map.png" alt="3D attention map" style="max-width: 82%; border-radius: 8px;">
            <div style="margin-top: 8px; font-size: 14px; color: #555; max-width: 82%; margin-left: auto; margin-right: auto;">
              <em>Figure 2: Illustration of 3D attention map, with diagonal bands from positional periodicity + sparse semantic spikes.</em>
            </div>
          </div>

          <p style="margin-top: 18px;">
            A useful consequence of our proposed attention map model is that, after a suitable permutation $\mathbf{P}$ that groups tokens based on spatiotemporal structure, the positional component $\mathbf{D}$ can become <em>blockwise rank-1</em>. For this reason, we explore the possibility of approximating these attention maps as Monarch matrices, which have sufficient representational power to capture such patterns with minimal error.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section: Monarch background -->
<section class="section hero is-light" id="monarch">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/Idea.png" style="height: 43px; display: inline; vertical-align:text-top;"/>
          &nbsp; Monarch + MonarchAttention: Background and Video Challenges
        </h2>

        <div class="content has-text-justified">
          <p>
            Monarch parameterization represents an $N\times N$ matrix (with $N=b_1 b_2$) as a structured factorization
            $$\mathbf{M} = \mathbf{P}\,\mathbf{L}\,\mathbf{P}^\top\,\mathbf{R},$$
            where $\mathbf{P}$ is a fixed permutation, and $\mathbf{L}$ and $\mathbf{R}$ are block-diagonal.
            Equivalently, $\mathbf{M}$ can be reshaped into a 4D tensor
            of shape $b_1\times b_2\times b_1\times b_2$ whose local slices are rank-$1$. Writing the blocked tensor as $\widetilde{\mathbf{M}}\in\mathbb{R}^{b_1\times b_2\times b_1\times b_2}$,
            $$\widetilde{\mathbf{M}}_{\ell j k i}=\mathbf{L}_{j\ell k}\,\mathbf{R}_{kji},$$
            so for fixed $(j,k)$, the slice $\widetilde{\mathbf{M}}_{:,j,k,:}$ is rank-$1$ in $(\ell,i)$.
            Choosing block sizes that align with spatiotemporal structure is crucial for effective Monarch approximation in video attention.
          </p>

          <h3 class="title is-4" style="margin-top: 22px;">MonarchAttention</h3>
          <p>
            <a href="https://arxiv.org/abs/2505.18698" style="color: #2c6eab; text-decoration: underline;"><strong>MonarchAttention</strong></a> (Yaras et al.) proposes to approximate the attention matrix
            $\mathbf{A}=\softmax(\mathbf{Q}\mathbf{K}^\top)\in\mathbb{R}^{N\times N}$ as a Monarch matrix
            by directly optimizing the Monarch factors via an <em>iterative alternating refinement</em> procedure.
            The key practical advantage is that one can update and apply factors $\mathbf{L}$ and $\mathbf{R}$ without
            explicitly forming $\mathbf{A}$.
          </p>

          <div style="text-align: center; margin-top: 14px;">
            <img src="static/images/monarch_f3.png" alt="MonarchAttention pipeline" style="max-width: 95%; border-radius: 8px;">
            <div style="margin-top: 8px; font-size: 14px; color: #555; max-width: 95%; margin-left: auto; margin-right: auto;">
              <em>Figure 3: MonarchAttention alternates factor updates to approximate $\softmax(\mathbf{Q}\mathbf{K}^\top)$ without materializing the full attention matrix.</em>
            </div>
          </div>

          <h3 class="title is-4" style="margin-top: 22px;">Challenges of Monarch Parameterization for Video Attention</h3>
            <p>We identify three key challenges of directly applying a MonarchAttention-style algorithm to video attention:</p>
          <div class="takeaway">
            <p style="margin-bottom: 8px;"><strong>C1 (Block Alignment).</strong> Picking generic block sizes $b_1$ and $b_2$ such that $N = b_1 b_2$ does <em>not</em> yield an accuracy-preserving sparse parameterization for video. Instead, there are strict block size constraints tied to the video latent dimensions (frames $f$, height $h$, width $w$) that must be satisfied for the parameterization to properly capture the positional structure.</p>
          </div>
          <div class="takeaway">
            <p style="margin-bottom: 8px;"><strong>C2 (Non-monotonic improvement with compute).</strong> Because of the alignment constraints, only a small set of block sizes actually “work” in practice. Critically, using block sizes outside this set does not necessarily improve approximation, even if the representation becomes less sparse. Not being able to reliably achieve higher accuracy by investing more compute makes the approach fairly inflexible.</p>
          </div>
          <div class="takeaway">
            <p style="margin-bottom: 0px;"><strong>C3 (Iteration overhead).</strong> A MonarchAttention-style alternating refinement procedure scales linearly with the number of refinement steps. In few-step diffusion and autoregressive video generation, this multi-iteration overhead can significantly mitigate any intended latency improvements.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section: Method -->
<section class="section hero is-light" id="method">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/Switch.png" style="height: 43px; display: inline; vertical-align:text-top;"/>
          &nbsp; MonarchRT: Solutions for Real-Time Video Attention
        </h2>

        <div class="content has-text-justified">

          <h3 class="title is-4">1) Block Alignment with Spatiotemporal Structure</h3>
          <p>
            For video DiTs, attention rows/columns correspond to physical visual patches, and a large fraction of the attention structure is
            driven by a separable positional component (the $\mathbf{D}$ term in our decomposition). For Monarch to <em>fully capture</em>
            this positional structure, the block sizes must be chosen so that the three positional terms can be placed cleanly into
            either $\mathbf{L}$ or $\mathbf{R}$ without splitting any spatiotemporal axis.
          </p>

          <p>
            This becomes concrete if we focus on representing the positional term
            $$\mathbf{D}_{(f_0,h_0,w_0),(f_1,h_1,w_1)} = d_f(f_0,f_1)\,d_h(h_0,h_1)\,d_w(w_0,w_1).$$
            Under the $(b_1,b_2)$ blocking, Monarch enforces a <em>blockwise rank-1</em> form
            $$\widetilde{\mathbf{M}}_{\ell j k i}=\mathbf{L}_{j\ell k}\,\mathbf{R}_{kji}.$$
            Exact representation of $\mathbf{D}$ is possible only if we can assign the three separable terms to the two Monarch factors without mixing axes:
            $\mathbf{L}$ must contain (a product of) some subset of $\{d_f,d_h,d_w\}$, and $\mathbf{R}$ must contain the remaining term(s).
            If a spatiotemporal axis is split across both block sizes (misalignment), then neither $\mathbf{L}$ nor $\mathbf{R}$ is indexed by <em>both coordinates</em> of that axis.
            (Here, "both coordinates" refers to the indices along a given axis for a query and key pair, e.g. $(w_0, w_1)$ along the width axis.)
            For example, if the width dimension were split between both block sizes, then one factor would be indexed by $w_0$ while the other is indexed by $w_1$, so the full 2D positional term $d_w(w_0,w_1)$ cannot live entirely inside a single factor.
            In that case, capturing $d_w$ exactly would require it to be rank-1 under the induced split, which is generally not true, so $\mathbf{D}$ cannot be represented exactly.
          </p>

          <div id="figure4" style="text-align: center; margin-top: 14px; margin-bottom: 20px;">
            <a href="#figure4">
              <img src="static/images/motivating_example_b.png" alt="Quality comparison of dense, top-k, and Monarch" style="max-width: 60%; border-radius: 8px;">
            </a>
            <div style="margin-top: 8px; font-size: 14px; color: #555; max-width: 60%; margin-left: auto; margin-right: auto;">
              <em>Figure 4: On Self-Forcing, Monarch parameterization can succeed (especially over oracle top-$k$), but only if block sizes are aligned with video dimensions; misalignment can introduce permutation-like artifacts.</em>
            </div>
          </div>

          <p>
            When each of $f$, $h$, and $w$ is fully contained in exactly one block size, the three separable terms can be assigned cleanly to
            $\mathbf{L}$ and $\mathbf{R}$. For example, with $(b_1,b_2)=(fh,w)$ one valid construction is
            $$\mathbf{L}_{w_0,(f_0,h_0),(f_1,h_1)}=d_f(f_0,f_1)\,d_h(h_0,h_1),\qquad \mathbf{R}_{(f_1,h_1),w_0,w_1}=d_w(w_0,w_1),$$
            which makes $\mathbf{M}$ reproduce $\mathbf{D}$ exactly.
          </p>

          <p>
            To generalize, for $N=f\,h\,w$, the alignment rule yields exactly six (nontrivial) aligned choices for $(b_1,b_2)$:
            $$ (fh,w),\ (w,fh),\ (f,hw),\ (hw,f),\ (fw,h),\ (h,fw). $$
          </p>

          <div class="takeaway">
            <strong>Takeaway:</strong> Misaligned block sizes prevent exact representation of the positional term $\mathbf{D}$, so an accurate parameterization requires obeying alignment of the Monarch block sizes with the spatiotemporal dimensions $f$, $h$, and $w$.
          </div>

          <h3 class="title is-4" style="margin-top: 28px;">2) Tiled Monarch Parameterization</h3>
          <p>
            From the alignment discussion above, there are only <strong>six</strong> block size configurations that exactly preserve the separable positional structure in video.
            This creates a practical limitation: if you want to <em>reduce sparsity</em> (i.e., spend more compute / parameters to improve approximation), the most obvious approach is to
            modify the block sizes $(b_1,b_2)$; however, this is unreliable, as block sizes outside the aligned set are <em>misaligned</em> with spatiotemporal axes, so they may produce worse approximations
            even when the representation is less sparse (see <a href="#figure4" style="color: #2c6eab; text-decoration: underline;">Figure 4</a>).
          </p>

          <p>
            MonarchRT introduces <em>tiled Monarch</em> to reduce sparsity while maintaining alignment: start from one of the six aligned base configurations, then subdivide the
            coarse rank-1 blocks induced by Monarch into smaller subtiles with factors $(c_1,c_2)$ (where $c_1\mid b_1$, $c_2\mid b_2$).
            So instead of estimating each <em>block</em> with a single set of rank-1 parameters, we estimate <em>each subtile</em> as its own rank-1 piece.
          </p>

          <p>
            More concretely, tiling replaces each original Monarch block with a grid of $c_1c_2$ subtiles (and does this across all blocks), where each subtile behaves like a
            smaller rank-1 block with effective sizes $\bigl(\tfrac{b_1}{c_1},\tfrac{b_2}{c_2}\bigr)$. This increases parameter count while maintaining expressivity through block size alignment.
          </p>

          <div style="text-align: center; margin-top: 14px; margin-bottom: 20px;">
            <img src="static/images/monarch_f4.png" alt="Tiled block sizes" style="max-width: 92%; border-radius: 8px;">
            <div style="margin-top: 8px; margin-bottom:16px; font-size: 14px; color: #555; max-width: 92%; margin-left: auto; margin-right: auto;">
              <em>Figure 5: Comparison of regular and Tiled Monarch. Tiled Monarch subdivides each block into smaller tiles, improving locality while keeping aligned structure.</em>
            </div>
          </div>

          <p>
            <strong>Tiled Monarch strictly generalizes untiled Monarch.</strong>
            For the same base block sizes $(b_1,b_2)$, untiled Monarch is a special case of tiled Monarch: set the subtile parameters so that all subtiles within an original block share the same rank-1 factors. Therefore,
            $$\mathcal{M}(b_1,b_2)\subseteq \mathcal{M}_{\mathrm{tile}}(b_1,b_2;c_1,c_2).$$
            Here $\mathcal{M}(b_1,b_2)$ denotes the set of (untiled) Monarch matrices with block sizes $(b_1,b_2)$, and $\mathcal{M}_{\mathrm{tile}}(b_1,b_2;c_1,c_2)$ denotes the set of tiled Monarch matrices with the same base block sizes and tiling factors $(c_1,c_2)$.
            When $c_1>1$ or $c_2>1$, this containment is strict: tiled Monarch can exactly parameterize certain matrices that violate the rank-$1$ constraint of an untiled block. Concretely, let $\boldsymbol{B}^{(j,k)}\in\mathbb{R}^{b_1\times b_2}$ denote the $(j,k)$-th untiled block, and focus on the specific block $\boldsymbol{B}^{(0,0)}$.
            Pick two coordinates $(r_1,s_1)$ and $(r_2,s_2)$ with $r_1\neq r_2$ and $s_1\neq s_2$ that lie in <em>different</em> subtiles of this same block, and set all subtiles to zero except those two; within each of the two chosen subtiles,
            set all entries to zero except at its designated coordinate $(r_1,s_1)$ or $(r_2,s_2)$, so that
            $$\boldsymbol{B}^{(0,0)}_{r_1,s_1}=1,\; \boldsymbol{B}^{(0,0)}_{r_2,s_2}=1,\; \text{and } \boldsymbol{B}^{(0,0)}_{r_1,s_2}=\boldsymbol{B}^{(0,0)}_{r_2,s_1}=0.$$
            Then the $2\times 2$ submatrix of $\boldsymbol{B}^{(0,0)}$ restricted to rows $\{r_1,r_2\}$ and columns $\{s_1,s_2\}$ is
            $$\begin{pmatrix}1 & 0\\ 0 & 1\end{pmatrix},$$
            hence $\mathrm{rank}(\boldsymbol{B}^{(0,0)})\ge 2$, which is impossible for untiled Monarch to exactly represent. But since every subtile is still rank-1, tiled Monarch can represent this construction exactly.
          </p>

          <div class="takeaway">
            <strong>Takeaway:</strong> Tiling restores a clear accuracy–efficiency tradeoff: increasing $(c_1,c_2)$ refines locality and improves approximation quality by investing more compute/parameters. Refining the granularity of each rank-1 partition also makes it easier to capture non-positional patterns, like sparse semantic correlations.
          </div>

          <div style="text-align: center; margin-top: 14px;">
            <img src="static/images/monarch_attn_code.png" alt="MonarchAttention and tiled MonarchAttention pseudocode" style="max-width: 95%; border-radius: 8px;">
            <div style="margin-top: 8px; font-size: 14px; color: #555; max-width: 95%; margin-left: auto; margin-right: auto;">
              <em>Figure 6: MonarchAttention vs. tiled MonarchAttention pseudocode: tiling modifies the refinement updates to operate on smaller sub-blocks.</em>
            </div>
          </div>

          <div class="note-box" style="margin-top: 14px;">
            <strong>Note:</strong>
            Another way to reduce sparsity while maintaining alignment is to estimate each original Monarch block as
            rank-$k$ for $k>1$. However, we find this does not yield a clean MonarchAttention-style
            iterative refinement algorithm for estimating the factors, so we do not adopt this approach.
          </div>

          <h3 class="title is-4" style="margin-top: 28px;">3) Finetuning + Triton Kernels (Reducing Iterative Overhead)</h3>
          <p>
            MonarchAttention uses iterative refinement to estimate factors $\mathbf{L}$ and $\mathbf{R}$.
            Using more iterations improves the accuracy of the approximation but linearly increases the runtime.
            MonarchRT reduces this overhead by employing finetuning, so that <strong>1 iteration + finetuning matches the quality of much longer refinement</strong> without finetuning, significantly lowering the algorithmic cost.
          </p>

          <div style="text-align: center; margin-top: 14px;">
            <img src="static/images/iterations.png" alt="Iterations vs. quality" style="max-width: 92%; border-radius: 8px;">
            <div style="margin-top: 8px; margin-bottom: 16px; font-size: 14px; color: #555; max-width: 92%; margin-left: auto; margin-right: auto;">
              <em>Figure 7: Iterative refinement improves quality but costs scale with iterations; finetuning recovers the quality of many iterations using 1.</em>
            </div>
          </div>

          <p>
            On the systems side, we write custom Triton kernels to efficiently implement the tiled MonarchAttention forward and backward passes,
            so the 1-iteration refinement cost remains small.
          </p>

          <div style="text-align: center; margin-top: 14px;">
            <img src="static/images/kernel_vis.png" alt="Kernel visualization" style="max-width: 92%; border-radius: 8px;">
            <div style="margin-top: 16px; font-size: 14px; color: #555; max-width: 92%; margin-left: auto; margin-right: auto;">
              <em>Figure 8: Two-stage kernel visualization for tiled MonarchAttention forward pass.</em>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section: Results -->
<section class="section hero is-light" id="results">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/experiments.png" style="height: 43px; display: inline; vertical-align:text-top;"/>
          &nbsp; Empirical Results (Quality + Speed)
        </h2>

        <div class="content has-text-justified">
          <h3 class="title is-4">Quality at High Sparsity</h3>
          <p>
            MonarchRT is designed for extreme regimes where quality cannot be traded for speed.
            On Self-Forcing, we reach <strong>95% attention sparsity</strong> while matching dense attention on VBench.
          </p>

          <div class="video-compare" style="margin-top: 10px;">
            <div class="video-compare__grid">
              <!-- Fixed left labels (rotated) -->
              <div class="video-compare__labels">
                <div class="video-compare__label"><span>Dense Attention</span></div>
                <div class="video-compare__label"><span>MonarchRT</span></div>
              </div>

              <!-- Horizontally scrolling strip -->
              <div class="video-compare__scroll" role="region" aria-label="Dense vs MonarchRT video comparisons">
                <div class="video-compare__strip">

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/000.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/000.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/001.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/001.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/002.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/002.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/003.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/003.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/004.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/004.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/005.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/005.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/006.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/006.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/007.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/007.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/008.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/008.mp4" type="video/mp4">
                    </video>
                  </div>

                  <div class="video-compare__col">
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/dense/009.mp4" type="video/mp4">
                    </video>
                    <video class="video-compare__vid" autoplay muted loop playsinline preload="metadata" controls>
                      <source src="static/videos/monarch/009.mp4" type="video/mp4">
                    </video>
                  </div>

                </div>
              </div>
            </div>

            <div style="margin-top: 8px; margin-bottom: 16px; font-size: 14px; color: #555; text-align: center;">
              <em>Example generations on autoregressive Self-Forcing, comparing dense attention and MonarchRT at 95% sparsity.</em>
            </div>
          </div>

          <table class="table is-striped is-fullwidth tight-table">
            <thead>
              <tr>
                <th>Method</th>
                <th style="text-align:right;">Quality</th>
                <th style="text-align:right;">Semantic</th>
                <th style="text-align:right;">Total</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Dense Attention</td>
                <td style="text-align:right;">0.844</td>
                <td style="text-align:right;">0.804</td>
                <td style="text-align:right;">0.836</td>
              </tr>
              <tr>
                <td><strong>MonarchRT (95% sparse)</strong></td>
                <td style="text-align:right;"><strong>0.846</strong></td>
                <td style="text-align:right;"><strong>0.805</strong></td>
                <td style="text-align:right;"><strong>0.838</strong></td>
              </tr>
            </tbody>
          </table>

          <p>
            The table above shows VBench scores on Self-Forcing for dense attention and MonarchRT at 95% (effective) sparsity. We inject MonarchRT directly into the distribution matching distillation (DMD) stage of Self-Forcing. Since MonarchRT reduces both inference and training costs, and since both models are trained for the same number of iterations, <strong>MonarchRT achieves higher performance even when using a smaller training budget than the dense model.</strong>
          </p>

          <h3 class="title is-4" style="margin-top: 22px;">Efficiency: Kernel and End-to-End Speed</h3>
          <p>
            With optimized kernels, MonarchRT speeds up attention substantially on Self-Forcing.
            Below we report measured 480p results at the same sparsity level (95%) used for the quality analysis above.
          </p>

          <h4 class="title is-5" style="margin-top: 12px;">Kernel Latency (Decode Final Frame)</h4>
          <table class="table is-striped is-fullwidth tight-table" style="margin-top: 10px;">
            <thead>
              <tr>
                <th>GPU</th>
                <th style="text-align:right;">FlashAttention (ms)</th>
                <th style="text-align:right;">MonarchRT (ms)</th>
                <th style="text-align:right;">Speedup</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>RTX 5090</td>
                <td style="text-align:right;">4.97</td>
                <td style="text-align:right;">1.27</td>
                <td style="text-align:right;">3.9×</td>
              </tr>
              <tr>
                <td>H100</td>
                <td style="text-align:right;">1.58</td>
                <td style="text-align:right;">1.04</td>
                <td style="text-align:right;">1.5×</td>
              </tr>
            </tbody>
          </table>

          <h4 class="title is-5" style="margin-top: 14px;">End-to-End Latency (Generate 81 Frames)</h4>
          <table class="table is-striped is-fullwidth tight-table" style="margin-top: 10px;">
            <thead>
              <tr>
                <th>GPU</th>
                <th style="text-align:right;">FlashAttention (ms)</th>
                <th style="text-align:right;">MonarchRT (ms)</th>
                <th style="text-align:right;">Speedup</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>RTX 5090</td>
                <td style="text-align:right;">8309.06</td>
                <td style="text-align:right;">6094.45</td>
                <td style="text-align:right;">1.36×</td>
              </tr>
              <tr>
                <td>H100</td>
                <td style="text-align:right;">3661.16</td>
                <td style="text-align:right;">3228.45</td>
                <td style="text-align:right;">1.13×</td>
              </tr>
            </tbody>
          </table>

          <p style="margin-top: 10px;">
            Furthermore, applying <span class="monospace">torch.compile</span> to MonarchRT at 95% sparsity yields an even higher practical end-to-end throughput gain:
            on RTX 5090 at 480p, FlashAttention-2 achieves <strong>11 FPS</strong> while MonarchRT achieves <strong>16 FPS</strong>, without requiring any additional lossy optimizations.
          </p>

          <p style="margin-top: 10px;">
            We can further push MonarchRT to achieve even higher theoretical speedups; for example, MonarchRT provides an 11.8$\times$ attention speedup over
            FA-2 on RTX 5090 for bidirectional attention with 720p video dimensions, at 98% effective sparsity. More detailed experiments are included in the paper.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section: Conclusion -->
<section class="section hero is-light" id="conclusion">
  <div class="container is-fluid">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="text-align: center;">
          <img src="static/images/icons/Telescope.png" style="height: 43px; display: inline; vertical-align:text-top;"/>
          &nbsp; Conclusion and Discussion
        </h2>

        <div class="content has-text-justified">
          <p>
            MonarchRT reframes efficient video attention: instead of assuming attention is sparse, we treat it as a
            mixture of separable positional structure, sparse semantic correspondences, and dense mixing;
            Monarch parameterizations provide the right inductive bias for this structure.
            By enforcing spatiotemporal block alignment, introducing tiled Monarch for monotonic refinement, and
            finetuning with custom kernels, MonarchRT delivers large attention speedups
            without compromising real-time video fidelity.
          </p>

          <p>
            MonarchRT is most useful when (i) attention is a dominant runtime term (typical for most video DiTs),
            (ii) the target regime is latency-sensitive (few-step diffusion, interactive generation), and
            (iii) quality must be preserved even at high sparsity.
            If the workload is already dominated by non-attention components, or if the model is tolerant to aggressive
            sparsity (e.g., highly redundant many-step diffusion), then simpler static or dynamic sparse methods may suffice.
          </p>

          <h3 class="title is-4" style="margin-top: 18px;">Citation</h3>
          <pre class="monospace" style="background: #fafafa; border: 1px solid #eee; border-radius: 6px; padding: 14px; overflow: auto;">@misc{agarwal2026monarchrtefficientattentionrealtime,
      title={MonarchRT: Efficient Attention for Real-Time Video Generation}, 
      author={Krish Agarwal and Zhuoming Chen and Cheng Luo and Yongqi Chen and Haizhong Zheng and Xun Huang and Atri Rudra and Beidi Chen},
      year={2026},
      eprint={2602.12271},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2602.12271}, 
}</pre>
        </div>

      </div>
    </div>
  </div>
</section>


  <footer class="footer">
    <div class="container is-fluid">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>. The icons are created by GPT4. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
